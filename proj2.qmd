```{r echo=FALSE}
library(factoextra)
library(tidyverse)
library(GGally)
library(caret)
library(lctools)
library(tidymodels)
```

# 1)

## a) on overleaf

half page summary of paper

## b) summary of data

Summarize the data, i.e., % of pixels for the different classes. Plot well-labeled beautiful maps using x, y coordinates the expert labels with color of the region based on the expert labels. Do you observe some trend/pattern? Is an i.i.d. assumption for the samples justified for this dataset?

```{r cache=TRUE}
# loading data 
img1 = data.frame(read.table("image_data/imagem1.txt"))
img2 = data.frame(read.table("image_data/imagem2.txt"))
img3 = data.frame(read.table("image_data/imagem3.txt"))

# name the columns  
col_names = c("y", "x", "label", "NDAI", "SD", "CORR", "DF","CF","BF","AF","AN")
colnames(img1) = col_names
colnames(img2) = col_names
colnames(img3) = col_names

# add source column to split into other sets 
img1$source = "img1"
img2$source = "img2"
img3$source = "img3"

# manipulation to make plotting easier 
lab_combined = rbind(img1[,c("x", "y", "label","source")],
                     img2[,c("x", "y", "label","source")],
                     img3[,c("x", "y", "label","source")])

# change to factors to color when plotting 
lab_combined$label = as.factor(lab_combined$label)

```

Geographic plot

```{r cache=TRUE}
ggplot(lab_combined, aes(x=x,y=y)) + 
  geom_point(aes(colour = label)) + 
  facet_grid(cols = vars(source)) + 
  labs(title = "Images With Expert Labels",
       subtitle = "-1: No Cloud | 0 : Uncertain | 1 : Cloud") + 
  scale_colour_brewer("Expert \nLabel")
```

```{r}
lab_combined2 = lab_combined %>%
  group_by(source, label) %>%
  count(label) %>%
  ungroup(label) %>%
  mutate(prop = n / sum(n))
```

```{r}
ggplot(lab_combined2, aes(label,fill=label)) + 
  geom_col(aes(y = prop), col = "darkgrey") + 
  facet_grid(cols = vars(source)) + 
  labs(title = "Images With Expert Labels",
       subtitle = "-1: No Cloud | 0 : Uncertain | 1 : Cloud") + 
  ylab("Proportion of Pixels") + 
  scale_fill_brewer("Expert \nLabel")
```
```{r}
#for reference
ggplot(lab_combined, aes(label,fill=label)) + 
  geom_bar() + 
  facet_grid(cols = vars(source)) + 
  labs(title = "Images With Expert Labels",
       subtitle = "-1: No Cloud | 0 : Uncertain | 1 : Cloud") + 
  ylab("Count of Pixels")
```

## c) EDA

For some basic EDA, I took a random sample of the total data to allow for quicker plotting. This should retain the structure of the data, but may remove some outliers.

```{r}
set.seed(1124)
img_combined = rbind(img1,img2,img3)
eda_sample = sample_n(img_combined,10000)
head(eda_sample)
```

PCA
should  label be in here?
```{r}
rand.pca = prcomp(eda_sample[,3:11], center=TRUE, scale = TRUE)
fviz_pca_biplot(rand.pca, geom="point", habillage = eda_sample$label)
```

Plotting some pairs

```{r}

# https://www.blopig.com/blog/2019/06/a-brief-introduction-to-ggpairs/
ggpairs(eda_sample, 
        columns=4:11, # all columns 
        ggplot2::aes(color=as.factor(label),alpha=0.2),
        progress = FALSE,
        lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.3)))
```
investigating other bits, maybe only look at cloud and no cloud,
```{r}
eda_sample %>%
  filter(label != 0) %>%
  group_by(label) %>%
  summarize(meanndai = mean(NDAI),
            minndai = min(NDAI),
            maxndai = max(NDAI))
```
```{r}
#density plot of that
eda_sample %>%
  filter(label == 1) %>%
  ggplot(aes(x = NDAI)) +
  geom_density(fill = 2, alpha = 0.5) + 
  geom_density(data = eda_sample %>% filter(label == -1), fill = 3, alpha = 0.5) +
  geom_density(data = eda_sample %>% filter(label == 0), fill = 1, alpha = 0.5)
```

```{r}
eda_sample %>%
  filter(label != 0) %>%
  group_by(label) %>%
  summarize(meanSD = mean(SD),
            minSD = min(SD),
            maxSD = max(SD))
```
This looks interesting, lets plot
```{r}
eda_sample %>%
  filter(label == 1) %>%
  ggplot(aes(x = CD)) +
  geom_density(fill = 2, alpha = 0.5) + 
  geom_density(data = eda_sample %>% filter(label == -1), fill = 3, alpha = 0.5) #+
  #geom_density(data = eda_sample %>% filter(label == 0), fill = 1, alpha = 0.5)
```

```{r}
eda_sample %>%
  filter(label != 0) %>%
  group_by(label) %>%
  summarize(meanCORR = mean(CORR),
            minCORR = min(CORR),
            maxCORR = max(CORR))
```
```{r}
#density plot of that
eda_sample %>%
  filter(label == 1) %>%
  ggplot(aes(x = CORR)) +
  geom_density(fill = 2, alpha = 0.5) + 
  geom_density(data = eda_sample %>% filter(label == -1), fill = 3, alpha = 0.5) #+
  #geom_density(data = eda_sample %>% filter(label == 0), fill = 1, alpha = 0.5)
```
```{r}
library(patchwork)
p1 = eda_sample %>%
  filter(label != 0) %>%
  ggplot() +
  geom_point(aes(x = NDAI, y = CORR, col = as.factor(label))) + 
  theme(legend.position = "none")


p2 = eda_sample %>%
  filter(label != 0) %>%
  ggplot() +
  geom_point(aes(x = NDAI, y = SD, col = as.factor(label))) + 
  theme(legend.position = "none")


p3 = eda_sample %>%
  filter(label != 0) %>%
  ggplot() +
  geom_point(aes(x = SD, y = CORR, col = as.factor(label)))+
  theme(legend.position = "none")

p1/(p2+p3)
```

#### analysis on the silly little angles
```{r}
head(eda_sample)
```

Notes on angular features: (re plot below) 
DF: very much overlap, -1 has most density in a smaller range than with 1
CF: upper values for -1, but vals still overlap
BF: upper values for -1, but vals still overlap (maybe more extreme than with CF)
AF: upper values for -1, but vals still overlap. -1 has a much much narrower range than 1
AN: upper values for -1, but vals still overlap. -1 has a much much narrower range than 1
```{r}
eda_sample %>%
  filter(label == 1) %>%
  #replace x = with feature of choosing
  ggplot(aes(x = AN, y = ..density..)) +
  geom_histogram(fill = 2, alpha = 0.5) + 
  geom_histogram(data = eda_sample %>% filter(label == -1), fill = 3, alpha = 0.5) #+
  #geom_density(data = eda_sample %>% filter(label == 0), fill = 1, alpha = 0.5)
```


# 2

## a)

The first, and most naive approach, would be to simply treat the three images as training, validation, and test data sets respectively. We know the data is not i.i.d, so it would be inappropriate to randomly sample points for our different sets.

Second, since the data is correlated, we can hand-pick slices from each of the three images that result in the desired split, but in a way that retains the ordering. If we split each image into three chunks, we can then choose the first, middle, and last chunk for each of the desired sets and combine them. If, for some reason, there is some kind of feature or interesting detail present in the "position" of a slice of the data, hopefully this will allow us to capture it without overstating accuracy in training. To do this, we calculated the indices by hand.

third: create a grid, not across images but training/validation/test in each of the splits and do that three times for each image? (lecture 13 / oct18). ex/ with 9 blocks, use 7 for training, one for validation , one for prediction, usual CV error. since we want to preserve spacial dependency.

blocked Cross validation, repeat process for the three images? what do we do w the 0 points? just remove?

Naive Approach

```{r}
# naive way 
naive_train = img1
naive_test = img2
naive_validate = img3
```

Hand split

```{r}

alt_train = data.frame(rbind(img1[23023:92089,],
                             img2[46093:115229,],
                             img3[1:69130,]))

alt_test = data.frame(rbind(img1[1:23022,],
                             img2[23047:46092,],
                             img3[92175:115217,]))

alt_validate = data.frame(rbind(img1[92090:115110,],
                             img2[1:23046,],
                             img3[69131:92174,]))
```

## b) 

The trivial classifier would have high accuracy if the image was mostly cloud-free. However, even so, this classifier is not catching clouds and is not being that meaningful, so it would be important to look at other metrics here.

```{r}
trivial = rep(-1,length(alt_test$label))
alt_triv_accuracy = sum(trivial == alt_test$label) / length(alt_test$label)
alt_validate_accuracy = sum(trivial == alt_validate$label) / length(alt_test$label)

alt_triv_accuracy
alt_validate_accuracy
```

## c) 
- want our data to be separated
- like lec16 on oct27 for classification error metrics?
- pairwise scatterplots for correlation?
- variable wise ... tree? like getting cut/off values of an X var to pred Y? i dont think that's too fancy

-in model pred, keep pt order
```{r}
head(img_combined)
```



## d) 
kfold cv loss

```{r}
CVmaster = function(classifier, feat, labels, folds, loss) {
  
}
```


# 4
note:be careful with
Hyperparameter choices
• dimension in PCA
• K in K-means, or GMM
• regularization parameter in regularlized methods
• number of trees in random forests

## a
```{r}
#CHANGE THIS WITH REAL NOT EDA SAMPLE (if we want to do tidymodels)
set.seed(1124)
indicies = sample(10000, 200)
train = eda_sample[-indicies,]
test = eda_sample[indicies,]

#cvs and recipie
train_cvs <- vfold_cv(train, v = 5)
train_recipie <- recipe(label ~ ., data = train)

train_recipie2 <- recipe(label ~ . data = train) %>%
  step_dummy(all_nominal()) %>%
  step_normalize(all_numeric(), -label)
```

Logistic Regression
```{r}
logreg_model = logistic_reg() %>%
  set_mode("classification") %>%
  set_engine("glm")
```

Logistic Regression w L1 penalty
```{r}

```

QDA
```{r}

```

tree
```{r}
#tree model
tree_mod <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_wflow <- workflow() %>%
  add_recipe(train_recipie) %>%
  add_model(tree_mod)

tree_fit <- tree_wflow %>%
  fit_resamples(train_cvs, metrics = metric_set(accuracy, roc_auc, precision, recall))

tree_fit %>% collect_metrics()
```

qda
```{r}

```

